{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing other dependencies\n",
    "import numpy as np\n",
    "# importing PyTorch\n",
    "import torch\n",
    "# import torch.nn Module\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# checks whether MPS is available\n",
    "print(torch.backends.mps.is_available())\n",
    "\n",
    "# this ensures that the current current PyTorch installation was built with MPS activated.\n",
    "print(torch.backends.mps.is_built())\n",
    "\n",
    "# setting the device to \"mps\" instead of default \"cpu\"\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demonstrate all the steps in sequence, we look at a Linear Regression model, trained and optimized via SGD.\n",
    "\n",
    "The training data consists of 4 points `(1,2) (2,4) (3,6) (4,8)`.\n",
    "\n",
    "We try to find the model: $y = f(x) = 2x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#samples: 4, #features: 1\n"
     ]
    }
   ],
   "source": [
    "# Training Sample, as Tensors\n",
    "X_train = torch.tensor([[1], [2], [3], [4]], dtype=torch.float32)\n",
    "y_train = torch.tensor([[2], [4], [6], [8]], dtype=torch.float32)\n",
    "\n",
    "n_samples, n_features =  X_train.shape\n",
    "print(f'#samples: {n_samples}, #features: {n_features}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Sample\n",
    "X_test = torch.tensor([[5], [6]], dtype=torch.float32)\n",
    "y_test = torch.tensor([[10], [12]], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we specify the no. of features and the output dim.\n",
    "input_dim = n_features\n",
    "output_dim = 1\n",
    "\n",
    "# we give out a single no. as output while predicting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Training Pipeline\n",
    "\n",
    "Given data and everything else, a full training pipeline consists of 3 steps:\n",
    "\n",
    "1. **Model Designing**: \n",
    "    - Input, Output\n",
    "    - Forward Pass (with different layers)\n",
    "2. **Loss and Optimizer**\n",
    "3. **Training Loop**:\n",
    "    - Forward: Compute Prediction and Loss\n",
    "    - Backward: Compute Gradients\n",
    "    - Update Weights (Parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Model Designing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we create custom wrappers to add multiple linear/non-linear layers in our model\n",
    "# this is enabled by torch.nn module\n",
    "\n",
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        # define different layers here:\n",
    "        self.lin = nn.Linear(input_dim, output_dim)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return self.lin(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise a model instance\n",
    "model = LinearRegression(input_dim, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: [f(5), f(6)] :\n",
      " tensor([[2.8919],\n",
      "        [3.3072]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# initially the weights are initialized randomly\n",
    "# we check the initial prediction on the test set before training\n",
    "\n",
    "print(\"Prediction before training: [f(5), f(6)] :\\n\", model(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Loss and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this scenario, we use the MSE Loss, with SGD optimization\n",
    "\n",
    "# learning rate\n",
    "lr = 0.01\n",
    "\n",
    "# no. of epochs\n",
    "num_epochs = 300\n",
    "\n",
    "# define MSE loss\n",
    "loss = nn.MSELoss()\n",
    "\n",
    "# define optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "# format of optimizer: torch.optim.SGD(weights, lr, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------\n",
      "Epoch#: 0\n",
      "Weights: Parameter containing:\n",
      "tensor([[1.7740]], requires_grad=True)\n",
      "Loss: tensor(0.0742, grad_fn=<MseLossBackward0>)\n",
      "-----------------------\n",
      "Epoch#: 20\n",
      "Weights: Parameter containing:\n",
      "tensor([[1.7871]], requires_grad=True)\n",
      "Loss: tensor(0.0658, grad_fn=<MseLossBackward0>)\n",
      "-----------------------\n",
      "Epoch#: 40\n",
      "Weights: Parameter containing:\n",
      "tensor([[1.7995]], requires_grad=True)\n",
      "Loss: tensor(0.0584, grad_fn=<MseLossBackward0>)\n",
      "-----------------------\n",
      "Epoch#: 60\n",
      "Weights: Parameter containing:\n",
      "tensor([[1.8112]], requires_grad=True)\n",
      "Loss: tensor(0.0518, grad_fn=<MseLossBackward0>)\n",
      "-----------------------\n",
      "Epoch#: 80\n",
      "Weights: Parameter containing:\n",
      "tensor([[1.8222]], requires_grad=True)\n",
      "Loss: tensor(0.0459, grad_fn=<MseLossBackward0>)\n",
      "-----------------------\n",
      "Epoch#: 100\n",
      "Weights: Parameter containing:\n",
      "tensor([[1.8325]], requires_grad=True)\n",
      "Loss: tensor(0.0407, grad_fn=<MseLossBackward0>)\n",
      "-----------------------\n",
      "Epoch#: 120\n",
      "Weights: Parameter containing:\n",
      "tensor([[1.8423]], requires_grad=True)\n",
      "Loss: tensor(0.0361, grad_fn=<MseLossBackward0>)\n",
      "-----------------------\n",
      "Epoch#: 140\n",
      "Weights: Parameter containing:\n",
      "tensor([[1.8515]], requires_grad=True)\n",
      "Loss: tensor(0.0320, grad_fn=<MseLossBackward0>)\n",
      "-----------------------\n",
      "Epoch#: 160\n",
      "Weights: Parameter containing:\n",
      "tensor([[1.8601]], requires_grad=True)\n",
      "Loss: tensor(0.0284, grad_fn=<MseLossBackward0>)\n",
      "-----------------------\n",
      "Epoch#: 180\n",
      "Weights: Parameter containing:\n",
      "tensor([[1.8682]], requires_grad=True)\n",
      "Loss: tensor(0.0252, grad_fn=<MseLossBackward0>)\n",
      "-----------------------\n",
      "Epoch#: 200\n",
      "Weights: Parameter containing:\n",
      "tensor([[1.8759]], requires_grad=True)\n",
      "Loss: tensor(0.0224, grad_fn=<MseLossBackward0>)\n",
      "-----------------------\n",
      "Epoch#: 220\n",
      "Weights: Parameter containing:\n",
      "tensor([[1.8831]], requires_grad=True)\n",
      "Loss: tensor(0.0198, grad_fn=<MseLossBackward0>)\n",
      "-----------------------\n",
      "Epoch#: 240\n",
      "Weights: Parameter containing:\n",
      "tensor([[1.8899]], requires_grad=True)\n",
      "Loss: tensor(0.0176, grad_fn=<MseLossBackward0>)\n",
      "-----------------------\n",
      "Epoch#: 260\n",
      "Weights: Parameter containing:\n",
      "tensor([[1.8963]], requires_grad=True)\n",
      "Loss: tensor(0.0156, grad_fn=<MseLossBackward0>)\n",
      "-----------------------\n",
      "Epoch#: 280\n",
      "Weights: Parameter containing:\n",
      "tensor([[1.9024]], requires_grad=True)\n",
      "Loss: tensor(0.0138, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    # forward pass:     \n",
    "    # compute prediction\n",
    "    y_train_hat = model(X_train)\n",
    "    # compute loss\n",
    "    train_loss = loss(y_train_hat, y_train)\n",
    "\n",
    "    # backward pass:\n",
    "    # compute gradients\n",
    "    train_loss.backward()\n",
    "\n",
    "    # update weights:\n",
    "    optimizer.step()\n",
    "    # zero-gradients after updating\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "\n",
    "    # output for debugging every 20 loops\n",
    "    if epoch%20 == 0:\n",
    "        # unpack the parameters\n",
    "        [W, b] = model.parameters()\n",
    "        print(\"-----------------------\")\n",
    "        print(\"Epoch#:\", epoch)\n",
    "        print(\"Weights:\", W)\n",
    "        print(\"Loss:\", train_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the weights converging to `2`, which means our model is slowing coverging towards $f(x) = 2x = w.x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction after training: [f(5), f(6)] :\n",
      " tensor([[ 9.8100],\n",
      "        [11.7178]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# we check the final prediction on the test set after training\n",
    "\n",
    "print(\"Prediction after training: [f(5), f(6)] :\\n\", model(X_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('conda_pytorch_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5e3ed40c8fb4c0778cc14e83fd08fd290bb35159f9df4bcbc4cc74f240b0dd9d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

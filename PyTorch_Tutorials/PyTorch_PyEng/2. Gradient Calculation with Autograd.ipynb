{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing other dependencies\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# importing PyTorch\n",
    "import torch\n",
    "\n",
    "# checks whether MPS is available\n",
    "print(torch.backends.mps.is_available())\n",
    "\n",
    "# this ensures that the current current PyTorch installation was built with MPS activated.\n",
    "print(torch.backends.mps.is_built())\n",
    "\n",
    "# setting the device to \"mps\" instead of default \"cpu\"\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Autograd Package\n",
    "\n",
    "The Autograd Package provides automatic differentiation for all operations on Tensors.\n",
    "\n",
    "Setting the argument `requires_grad = True` in a tensor, tracks all the operations on the corresponding tensor, so that the gradient of the tensor could be called out at any step (via Backpropagation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Calculating the gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2020, 0.1253, 0.0450], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# a tensor with required_grad set to True\n",
    "x = torch.rand(3, requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.2020, 2.1253, 2.0450], grad_fn=<AddBackward0>)\n",
      "<AddBackward0 object at 0x10a6d9cc0>\n",
      "tensor([0.4040, 0.2506, 0.0900], grad_fn=<MulBackward0>)\n",
      "<MulBackward0 object at 0x11e6e0700>\n"
     ]
    }
   ],
   "source": [
    "# we do various operations on x and set it to variable y check the gradients\n",
    "# since y was created as a result of an operation on x, it has a grad_fn attribute\n",
    "# grad_fn references a funcition that has created the tensor\n",
    "\n",
    "# addition\n",
    "y = x+2\n",
    "print(y)\n",
    "print(y.grad_fn)\n",
    "# grad_fn: AddBackward\n",
    "\n",
    "# multiplication\n",
    "y = 2*x\n",
    "print(y)\n",
    "print(y.grad_fn)\n",
    "# grad_fn: MulBackward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now compute the gradients with backpropagation. When we finish our computation we can call `.backward()` and have all the gradients computed automatically.\n",
    "\n",
    "The gradient for this tensor will be accumulated into `.grad` attribute. It is the partial derivate of the function w.r.t. the tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0585, grad_fn=<MeanBackward0>)\n",
      "<MeanBackward0 object at 0x105d5fc10>\n",
      "tensor([0.4040, 0.2506, 0.0900])\n"
     ]
    }
   ],
   "source": [
    "# We can calculate gradients of a function (say z) w.r.t. x (i.e. dz/dx) via backpropagation, by calling `z.backward()` and then calling `x.grad`\n",
    "y = 3*x*x\n",
    "z = y.mean()\n",
    "print(z)\n",
    "print(z.grad_fn)\n",
    "\n",
    "# calculating dz/dx\n",
    "z.backward()\n",
    "print(x.grad)\n",
    "\n",
    "# z.backward() doesn't require an argument in this case because the function has only one output (gradient is calculated at the value of the point 'x', which we have initialized). \n",
    "\n",
    "# If z gave a multi-variate output, then we have to specify an argument (input) to output the gradient by multiplying the input to the Jacobian matrix to get the output (w.r.t. the point 'x', which we have initialized).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.autograd` is an engine for computing *vector-Jacobian* product. It computes partial derivates while applying the chain rule.\n",
    "\n",
    "*Model with scalar output (exactly 1 element as output):*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0408, 0.0157, 0.0020], grad_fn=<DivBackward0>)\n",
      "<DivBackward0 object at 0x105d996c0>\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "grad can be implicitly created only for scalar outputs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/sampadk04/Desktop/Programming/VSCode-Projects/Python/PyTorch_PyEng/notebooks/Gradient_Calculation_with_Autograd.ipynb Cell 10'\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sampadk04/Desktop/Programming/VSCode-Projects/Python/PyTorch_PyEng/notebooks/Gradient_Calculation_with_Autograd.ipynb#ch0000009?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(z\u001b[39m.\u001b[39mgrad_fn)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sampadk04/Desktop/Programming/VSCode-Projects/Python/PyTorch_PyEng/notebooks/Gradient_Calculation_with_Autograd.ipynb#ch0000009?line=5'>6</a>\u001b[0m \u001b[39m# calculating dz.dx\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/sampadk04/Desktop/Programming/VSCode-Projects/Python/PyTorch_PyEng/notebooks/Gradient_Calculation_with_Autograd.ipynb#ch0000009?line=6'>7</a>\u001b[0m z\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sampadk04/Desktop/Programming/VSCode-Projects/Python/PyTorch_PyEng/notebooks/Gradient_Calculation_with_Autograd.ipynb#ch0000009?line=7'>8</a>\u001b[0m \u001b[39mprint\u001b[39m(x\u001b[39m.\u001b[39mgrad)\n",
      "File \u001b[0;32m/Volumes/SKK-T7/Apps/conda_envs/conda_pytorch_env/lib/python3.10/site-packages/torch/_tensor.py:401\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    393\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    394\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    395\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    399\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    400\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 401\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m/Volumes/SKK-T7/Apps/conda_envs/conda_pytorch_env/lib/python3.10/site-packages/torch/autograd/__init__.py:184\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    180\u001b[0m inputs \u001b[39m=\u001b[39m (inputs,) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(inputs, torch\u001b[39m.\u001b[39mTensor) \u001b[39melse\u001b[39;00m \\\n\u001b[1;32m    181\u001b[0m     \u001b[39mtuple\u001b[39m(inputs) \u001b[39mif\u001b[39;00m inputs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mtuple\u001b[39m()\n\u001b[1;32m    183\u001b[0m grad_tensors_ \u001b[39m=\u001b[39m _tensor_or_tensors_to_tuple(grad_tensors, \u001b[39mlen\u001b[39m(tensors))\n\u001b[0;32m--> 184\u001b[0m grad_tensors_ \u001b[39m=\u001b[39m _make_grads(tensors, grad_tensors_, is_grads_batched\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    185\u001b[0m \u001b[39mif\u001b[39;00m retain_graph \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    186\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n",
      "File \u001b[0;32m/Volumes/SKK-T7/Apps/conda_envs/conda_pytorch_env/lib/python3.10/site-packages/torch/autograd/__init__.py:85\u001b[0m, in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[39mif\u001b[39;00m out\u001b[39m.\u001b[39mrequires_grad:\n\u001b[1;32m     84\u001b[0m     \u001b[39mif\u001b[39;00m out\u001b[39m.\u001b[39mnumel() \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m---> 85\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mgrad can be implicitly created only for scalar outputs\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     86\u001b[0m     new_grads\u001b[39m.\u001b[39mappend(torch\u001b[39m.\u001b[39mones_like(out, memory_format\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mpreserve_format))\n\u001b[1;32m     87\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"
     ]
    }
   ],
   "source": [
    "y = 3*x*x\n",
    "z = y/3\n",
    "print(z)\n",
    "print(z.grad_fn)\n",
    "\n",
    "# calculating dz.dx\n",
    "z.backward()\n",
    "print(x.grad)\n",
    "# this gives an error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Model with non-scalar output:*\n",
    "\n",
    "- If a Tensor is non-scalar (more than 1 elements), we need to specify *arguments* for `backward()` specify a gradient argument that is a tensor of matching shape (which is needed for vector-Jacobian product)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0408, 0.0157, 0.0020], grad_fn=<DivBackward0>)\n",
      "<DivBackward0 object at 0x11e7fd9c0>\n",
      "tensor([0.8081, 0.5012, 0.1800])\n"
     ]
    }
   ],
   "source": [
    "y = 3*x*x\n",
    "z = y/3\n",
    "print(z)\n",
    "print(z.grad_fn)\n",
    "\n",
    "# calculating gradient by multiplying the argument by the Jacobian matrix\n",
    "z.backward(torch.IntTensor([1,1,1]))\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Stop tracking history of gradients\n",
    "\n",
    "During our training loop when we want to update our weights. This update operation should not be part of the gradient computation. We can make sure to not keep track of this calculation (to exclude this in gradient calculations) by using one of the following methods:\n",
    "- `x.requires_grad_(False)`\n",
    "- `x.detach()`\n",
    "- wrap in `with torch.no_grad():`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.1** `.requires_grad_(..)` changes an existing flag in-place:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2,2)\n",
    "print(a.requires_grad)\n",
    "\n",
    "b = ((a*3)/(a-1))\n",
    "print(b.requires_grad)\n",
    "print(b.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "<SumBackward0 object at 0x1225b3520>\n"
     ]
    }
   ],
   "source": [
    "a.requires_grad_(True)\n",
    "print(a.requires_grad)\n",
    "\n",
    "b = (a*a).sum()\n",
    "print(b.requires_grad)\n",
    "print(b.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.2** `.detach()` get a new Tensor with the same content but no gradient computation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2, 2, requires_grad=True)\n",
    "print(a.requires_grad)\n",
    "\n",
    "b = a.detach()\n",
    "print(b.requires_grad)\n",
    "\n",
    "# in-place\n",
    "a.detach_()\n",
    "print(a.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.3** wrap in `with torch.no_grad()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2, 2, requires_grad=True)\n",
    "print(a.requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    b = (a * 2).sum()\n",
    "    print(b.requires_grad)\n",
    "    print(b.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.4** Why do we need to stop tracking the history of gradients?\n",
    "\n",
    "The `.backward()` call accumulates (updates the gradient by adding the new gradient to the previous existing gradient) the gradient for this tensor into `.grad` attribute.\n",
    "\n",
    "**!!! Need to be careful during optimization !!!**\n",
    "\n",
    "Use `.zero_()` to empty the gradients before a *new optimization step*!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1.], requires_grad=True)\n",
      "tensor([3., 3., 3., 3.])\n",
      "tensor([6., 6., 6., 6.])\n",
      "tensor([9., 9., 9., 9.])\n"
     ]
    }
   ],
   "source": [
    "weights = torch.ones(4, requires_grad=True)\n",
    "print(weights)\n",
    "\n",
    "# this instance is without resetting the grads to zero\n",
    "\n",
    "for epoch in range(3):\n",
    "    # dummy operation\n",
    "    model_output = (weights*3).sum()\n",
    "    # computes gradients w.r.t. model_output\n",
    "    model_output.backward()\n",
    "\n",
    "    # check the gradient in each epoch\n",
    "    print(weights.grad)\n",
    "    # notice the grad keeps on adding (accumulating) and hence, this can't be used to update the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1.], requires_grad=True)\n",
      "tensor([3., 3., 3., 3.])\n",
      "tensor([3., 3., 3., 3.])\n",
      "tensor([3., 3., 3., 3.])\n",
      "tensor([0.1000, 0.1000, 0.1000, 0.1000], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "weights = torch.ones(4, requires_grad=True)\n",
    "print(weights)\n",
    "\n",
    "# this instance with resetting the grads to zero\n",
    "\n",
    "for epoch in range(3):\n",
    "    # dummy operation\n",
    "    model_output = (weights*3).sum()\n",
    "    # computes gradients w.r.t. model_output\n",
    "    model_output.backward()\n",
    "\n",
    "    print(weights.grad)\n",
    "\n",
    "    # we can also choose to update the weights using a learning rate parameter inside a `torch.no_grad()` wrapper (this is to prevent this operation to participate in the gradient calculation)\n",
    "    with torch.no_grad():\n",
    "        # updating the weights\n",
    "        weights -= 0.1 * weights.grad\n",
    "\n",
    "    # resetting the gradients to zero in-place (to stop the accumulation of gradients)\n",
    "    weights.grad.zero_()\n",
    "\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Optimizer` has zero_grad() method. Example:\n",
    "```Python\n",
    "optimizer = torch.optim.SGD([weights], lr=0.1)\n",
    "# During training:\n",
    "optimizer.step()\n",
    "optimizer.zero_grad()\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('conda_pytorch_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5e3ed40c8fb4c0778cc14e83fd08fd290bb35159f9df4bcbc4cc74f240b0dd9d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
